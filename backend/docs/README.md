# Sri Lankan Situational Awareness Platform

A comprehensive real-time situational awareness platform for Sri Lanka that processes news, trends, weather, food pricing, tax revenue, YouTube, and national signals into actionable insights, indicators, risks, trends, and anomalies.

## ğŸ—ï¸ System Architecture

### Technology Stack
- **Backend Framework**: Flask 2.3.3
- **Task Queue**: Celery 5.3.4 with Redis broker
- **Database**: MongoDB with PyMongo
- **Data Processing**: Pandas, NumPy, SciPy
- **Machine Learning**: Scikit-learn, NLTK
- **Containerization**: Docker, Docker Compose
- **Monitoring**: Flower for Celery monitoring

### Data Flow Pipeline
```
Scraping â†’ Ingestion â†’ Preprocessing â†’ Analysis â†’ API Exposure
```

## ğŸ“ Project Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config/                 # Configuration files
â”‚   â”‚   â”œâ”€â”€ app_config.py       # Flask application configuration
â”‚   â”‚   â”œâ”€â”€ celery_app.py       # Celery application setup
â”‚   â”‚   â”œâ”€â”€ mongo_config.py     # MongoDB connection configuration
â”‚   â”‚   â””â”€â”€ settings.py         # Application settings
â”‚   â”œâ”€â”€ ml/                     # Machine learning models
â”‚   â”‚   â”œâ”€â”€ anomaly_detector.py # Anomaly detection algorithms
â”‚   â”‚   â”œâ”€â”€ clustering_engine.py # Data clustering models
â”‚   â”‚   â”œâ”€â”€ feature_engineer.py # Feature engineering utilities
â”‚   â”‚   â”œâ”€â”€ trend_analyzer.py   # Trend analysis integration
â”‚   â”‚   â””â”€â”€ trend_scorer.py     # Trend scoring algorithms
â”‚   â”œâ”€â”€ model/                  # MongoDB data models
â”‚   â”‚   â”œâ”€â”€ indicator_model.py  # Indicator data model
â”‚   â”‚   â”œâ”€â”€ insight_model.py    # Insight data model
â”‚   â”‚   â”œâ”€â”€ news_model.py       # News article model
â”‚   â”‚   â”œâ”€â”€ pricing_model.py    # Food pricing model
â”‚   â”‚   â”œâ”€â”€ risk_model.py       # Risk assessment model
â”‚   â”‚   â”œâ”€â”€ tax_model.py        # Tax revenue model
â”‚   â”‚   â”œâ”€â”€ trends_model.py     # Trends data model
â”‚   â”‚   â”œâ”€â”€ weather_model.py    # Weather data model
â”‚   â”‚   â””â”€â”€ youtube_model.py    # YouTube data model
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ ScrapModule/        # Data collection modules
â”‚   â”‚   â”‚   â”œâ”€â”€ NewsScrapper.py # News data collector
â”‚   â”‚   â”‚   â”œâ”€â”€ foodPricingScrap.py # Food pricing collector
â”‚   â”‚   â”‚   â”œâ”€â”€ google_trends_collector.py # Google Trends collector
â”‚   â”‚   â”‚   â”œâ”€â”€ taxRevenueGather.py # Tax revenue collector
â”‚   â”‚   â”‚   â”œâ”€â”€ weatherCollector.py # Weather data collector
â”‚   â”‚   â”‚   â””â”€â”€ youtube_collector.py # YouTube data collector
â”‚   â”‚   â”œâ”€â”€ ingestionLayer/     # Data ingestion pipeline
â”‚   â”‚   â”‚   â”œâ”€â”€ data_ingestor.py # Data ingestion utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ ingestion_pipeline.py # Main ingestion pipeline
â”‚   â”‚   â”‚   â””â”€â”€ scheduler.py    # Ingestion scheduling
â”‚   â”‚   â””â”€â”€ preprocessingLayer/ # Data preprocessing
â”‚   â”‚       â”œâ”€â”€ data_cleaner.py # Data cleaning utilities
â”‚   â”‚       â”œâ”€â”€ normalization_engine.py # Data normalization
â”‚   â”‚       â”œâ”€â”€ preprocessing_pipeline.py # Main preprocessing pipeline
â”‚   â”‚       â””â”€â”€ text_preprocessor.py # Text preprocessing
â”‚   â”œâ”€â”€ routes/                 # API routes
â”‚   â”‚   â””â”€â”€ api_routes.py       # All REST API endpoints
â”‚   â”œâ”€â”€ service/
â”‚   â”‚   â”œâ”€â”€ general/            # General services
â”‚   â”‚   â””â”€â”€ tasks/              # Celery task definitions
â”‚   â”‚       â”œâ”€â”€ analysis_tasks.py # Analysis tasks
â”‚   â”‚       â”œâ”€â”€ processing_tasks.py # Processing tasks
â”‚   â”‚       â””â”€â”€ scraping_tasks.py # Scraping tasks
â”‚   â””â”€â”€ __init__.py            # Application initialization
â”œâ”€â”€ docs/                      # Documentation
â”‚   â””â”€â”€ README.md              # This file
â”œâ”€â”€ Dockerfile                 # Flask application container
â”œâ”€â”€ Dockerfile.worker          # Celery worker container
â”œâ”€â”€ docker-compose.yml         # Docker compose configuration
â””â”€â”€ requirements.txt           # Python dependencies
```

## ğŸš€ Quick Start

### Prerequisites
- Docker and Docker Compose
- Python 3.11+ (for development)

### Running with Docker Compose

1. **Clone and setup**:
   ```bash
   git clone <repository-url>
   cd backend
   ```

2. **Start all services**:
   ```bash
   docker-compose up -d
   ```

3. **Access services**:
   - Flask API: http://localhost:5000
   - Flower (Celery monitoring): http://localhost:5555
   - MongoDB: localhost:27017
   - Redis: localhost:6379

### Manual Setup (Development)

1. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

2. **Download NLTK data**:
   ```python
   import nltk
   nltk.download('punkt')
   nltk.download('stopwords')
   nltk.download('vader_lexicon')
   ```

3. **Set environment variables**:
   ```bash
   export FLASK_ENV=development
   export MONGODB_URI=mongodb://localhost:27017/situational_awareness
   export REDIS_URL=redis://localhost:6379/0
   ```

4. **Start services**:
   ```bash
   # Start Redis
   redis-server
   
   # Start MongoDB
   mongod
   
   # Start Flask application
   flask run
   
   # Start Celery worker
   celery -A app.celery worker --loglevel=info
   
   # Start Celery beat (scheduler)
   celery -A app.celery beat --loglevel=info
   ```

## ğŸ“Š Data Sources & Collection

### News Data
- **Sources**: Daily Mirror, Sunday Times, NewsFirst, Ada Derana
- **Collection**: Web scraping with BeautifulSoup
- **Frequency**: Hourly
- **Data Points**: Headlines, content, publication date, categories

### Google Trends
- **Sources**: Google Trends API
- **Collection**: API integration with fallback simulation
- **Frequency**: Daily
- **Data Points**: Search trends, interest over time, related queries

### YouTube Data
- **Sources**: YouTube Data API
- **Collection**: API integration with fallback simulation
- **Frequency**: Daily
- **Data Points**: Video metrics, engagement, comments sentiment

### Weather Data
- **Sources**: OpenWeatherMap API
- **Collection**: API integration with fallback simulation
- **Frequency**: 3-hour intervals
- **Data Points**: Temperature, humidity, precipitation, wind speed

### Food Pricing
- **Sources**: Market data, government reports
- **Collection**: Web scraping and manual data entry simulation
- **Frequency**: Weekly
- **Data Points**: Commodity prices, market trends, supply indicators

### Tax Revenue
- **Sources**: Government revenue reports
- **Collection**: Data scraping and simulation
- **Frequency**: Monthly
- **Data Points**: Tax categories, revenue amounts, trends

## ğŸ”§ API Endpoints

### Health & Status
- `GET /status/health` - System health check
- `GET /status/queues` - Celery queue status

### Pipeline Control
- `POST /scrape/run` - Trigger scraping pipeline
- `POST /preprocess/run` - Trigger preprocessing pipeline
- `POST /analyze/run` - Trigger analysis pipeline

### Data Access
- `GET /indicators/latest` - Latest situational indicators
- `GET /risks/latest` - Current risk assessments
- `GET /insights/overview` - Comprehensive insights overview
- `GET /trends` - Current trends across all data types

### Query Parameters
- `type` - Filter by data type (news, weather, prices, tax, youtube, all)
- `lookback` - Number of days to look back (default: 30)
- `limit` - Number of results to return

## ğŸ¤– Machine Learning Features

### Anomaly Detection
- **Univariate Methods**: Z-score, IQR (Interquartile Range)
- **Multivariate Methods**: Isolation Forest
- **Sri Lankan Context**: Custom thresholds for local patterns

### Trend Analysis
- **Composite Scoring**: Slope, R-squared, momentum, volatility
- **Weighted Averages**: Sri Lankan context-specific weights
- **Cross-Domain Correlation**: Weather â†’ prices, news â†’ sentiment

### Clustering
- **Algorithms**: KMeans with silhouette score optimization
- **Applications**: News categorization, price pattern grouping
- **Sri Lankan Features**: Local stopwords, regional patterns

### Feature Engineering
- **Temporal Features**: Day of week, hour, fiscal periods
- **Rolling Statistics**: Moving averages, standard deviations
- **Cross-Domain Features**: Weather-impacted pricing, sentiment-driven trends

## ğŸ—ƒï¸ Database Schema

### Collections
- `news_articles` - Raw and processed news data
- `weather_data` - Meteorological observations
- `food_prices` - Commodity pricing information
- `tax_revenue` - Government revenue data
- `youtube_metrics` - Social media engagement data
- `trends_data` - Processed trend information
- `indicators` - Calculated situational indicators
- `risks` - Risk assessment results
- `insights` - Business intelligence insights

### Indexes
- Time-based indexes for all collections
- Category indexes for efficient filtering
- Text indexes for search functionality

## âš™ï¸ Configuration

### Environment Variables
```bash
# Flask Configuration
FLASK_ENV=production
FLASK_DEBUG=0

# MongoDB Configuration
MONGODB_URI=mongodb://mongodb:27017/situational_awareness
MONGODB_DB_NAME=situational_awareness

# Redis Configuration
REDIS_URL=redis://redis:6379/0

# API Keys (for production)
NEWS_API_KEY=your_news_api_key
YOUTUBE_API_KEY=your_youtube_api_key
OPENWEATHER_API_KEY=your_weather_api_key
GOOGLE_TRENDS_API_KEY=your_trends_api_key
```

### Celery Configuration
- **Broker**: Redis
- **Result Backend**: Redis
- **Task Queues**: scraping, processing, analysis
- **Concurrency**: 4 workers per queue
- **Scheduled Tasks**: Hourly scraping, daily analysis

## ğŸ§ª Testing

### Running Tests
```bash
# Run all tests
python -m pytest tests/

# Run specific test module
python -m pytest tests/test_scrapers.py

# Run with coverage
python -m pytest --cov=app tests/
```

### Test Structure
```
tests/
â”œâ”€â”€ test_scrapers.py          # Data collector tests
â”œâ”€â”€ test_ingestion.py         # Ingestion pipeline tests
â”œâ”€â”€ test_preprocessing.py     # Preprocessing tests
â”œâ”€â”€ test_ml_models.py         # Machine learning tests
â”œâ”€â”€ test_api_routes.py        # API endpoint tests
â””â”€â”€ conftest.py               # Test configuration
```

## ğŸ“ˆ Monitoring & Logging

### Application Logging
- **Level**: INFO for production, DEBUG for development
- **Format**: JSON structured logging
- **Output**: Console and file rotation

### Performance Monitoring
- **Celery**: Flower dashboard at http://localhost:5555
- **MongoDB**: Built-in monitoring and profiling
- **Redis**: Redis CLI monitoring commands
- **Application**: Custom metrics endpoint

### Health Checks
- **Database Connectivity**: MongoDB ping
- **Queue Health**: Celery worker status
- **API Responsiveness**: Endpoint response times

## ğŸš¢ Deployment

### Production Deployment

1. **Build and push containers**:
   ```bash
   docker-compose build
   docker-compose push
   ```

2. **Deploy to orchestration**:
   ```bash
   # Kubernetes example
   kubectl apply -f kubernetes/
   ```

3. **Configure production environment**:
   ```bash
   # Set production environment variables
   export FLASK_ENV=production
   export MONGODB_URI=mongodb://production-mongodb:27017/situational_awareness
   export REDIS_URL=redis://production-redis:6379/0
   ```

### Scaling Considerations
- **Horizontal Scaling**: Multiple Celery workers
- **Database**: MongoDB replica sets
- **Caching**: Redis cluster for high availability
- **Load Balancing**: Multiple Flask instances behind load balancer

## ğŸ”’ Security

### Data Protection
- **Encryption**: TLS for all external communications
- **Authentication**: API key authentication for external services
- **Authorization**: Role-based access control for internal APIs

### Vulnerability Management
- **Dependency Scanning**: Regular security updates
- **Code Analysis**: Static security analysis
- **Penetration Testing**: Regular security assessments

### Compliance
- **Data Retention**: Configurable retention policies
- **Audit Logging**: Comprehensive activity logging
- **Privacy**: Anonymization of personal data

## ğŸ¤ Contributing

### Development Workflow

1. **Fork the repository**
2. **Create feature branch**: `git checkout -b feature/amazing-feature`
3. **Commit changes**: `git commit -m 'Add amazing feature'`
4. **Push to branch**: `git push origin feature/amazing-feature`
5. **Open Pull Request**

### Code Standards
- **Python**: PEP 8 compliance
- **Documentation**: Google-style docstrings
- **Testing**: 80%+ test coverage
- **Type Hints**: Comprehensive type annotations

## ğŸ“ Support

### Documentation
- This README
- API documentation at `/docs` endpoint
- Code comments and docstrings

### Issue Tracking
- GitHub Issues for bug reports
- Feature requests via Pull Requests
- Security vulnerabilities: security@example.com

### Community
- Discord channel for developers
- Regular community meetings
- Contributor recognition program

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Sri Lankan government open data initiatives
- Open source community contributions
- Research institutions supporting situational awareness
- Development team and contributors

---

**Last Updated**: December 2024
**Version**: 1.0.0
**Status**: Production Ready